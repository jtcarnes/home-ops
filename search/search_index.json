{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 Welcome to the documentation for my home-ops repo. This repository is my home Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a GitHub issue or join the k8s@home Discord if you have any questions. Thanks \u00b6 A lot of inspiration for my cluster came from the people that have shared their clusters over at the awesome-home-kubernetes repository.","title":"Welcome"},{"location":"#welcome","text":"Welcome to the documentation for my home-ops repo. This repository is my home Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a GitHub issue or join the k8s@home Discord if you have any questions.","title":"Welcome"},{"location":"#thanks","text":"A lot of inspiration for my cluster came from the people that have shared their clusters over at the awesome-home-kubernetes repository.","title":"Thanks"},{"location":"gitops/","text":"Storage \u00b6","title":"Storage"},{"location":"gitops/#storage","text":"","title":"Storage"},{"location":"home/cluster_overview/","text":"Cluster overview \u00b6 My cluster is k3s provisioned on Ubuntu 21.04 nodes using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes. See my ansible directory for my playbooks and roles. Hardware \u00b6 Device Count OS Disk Size Data Disk Size Ram Purpose Intel NUC8i3BEH 1 512GB SSD 1TB NVMe (rook-ceph) 32GB k3s Masters (embedded etcd) Intel NUC8i5BEH 2 512GB SSD 1TB NVMe (rook-ceph) 32GB k3s Masters (embedded etcd) Lenovo ThinkCentre M93p Tiny 1 256GB SSD N/A 8GB k3s Workers Synology DS918+ 1 N/A 3x6TB + 1x10TB SHR 16GB Shared file storage Kubernetes cluster components \u00b6 GitOps \u00b6 flux : Keeps the cluster in sync with this Git repository. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository. Networking \u00b6 See here for more information about my storage setup. calico : For internal cluster networking. kube-vip : Uses BGP to load balance the control-plane API, making it highly availible without requiring external HA proxy solutions. metallb : Uses layer 2 to provide Load Balancing features to my cluster. external-dns : Creates DNS entries in a separate CoreDNS deployment which lives on my router. Multus : For allowing pods to have multiple network interfaces. cert-manager : Configured to create TLS certs for all ingress services automatically using LetsEncrypt . Storage \u00b6 See here for more information about my storage setup. rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage or CephFS storage. Kasten K10 : Data backup and recovery","title":"Cluster overview"},{"location":"home/cluster_overview/#cluster-overview","text":"My cluster is k3s provisioned on Ubuntu 21.04 nodes using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes. See my ansible directory for my playbooks and roles.","title":"Cluster overview"},{"location":"home/cluster_overview/#hardware","text":"Device Count OS Disk Size Data Disk Size Ram Purpose Intel NUC8i3BEH 1 512GB SSD 1TB NVMe (rook-ceph) 32GB k3s Masters (embedded etcd) Intel NUC8i5BEH 2 512GB SSD 1TB NVMe (rook-ceph) 32GB k3s Masters (embedded etcd) Lenovo ThinkCentre M93p Tiny 1 256GB SSD N/A 8GB k3s Workers Synology DS918+ 1 N/A 3x6TB + 1x10TB SHR 16GB Shared file storage","title":"Hardware"},{"location":"home/cluster_overview/#kubernetes-cluster-components","text":"","title":"Kubernetes cluster components"},{"location":"home/cluster_overview/#gitops","text":"flux : Keeps the cluster in sync with this Git repository. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository.","title":"GitOps"},{"location":"home/cluster_overview/#networking","text":"See here for more information about my storage setup. calico : For internal cluster networking. kube-vip : Uses BGP to load balance the control-plane API, making it highly availible without requiring external HA proxy solutions. metallb : Uses layer 2 to provide Load Balancing features to my cluster. external-dns : Creates DNS entries in a separate CoreDNS deployment which lives on my router. Multus : For allowing pods to have multiple network interfaces. cert-manager : Configured to create TLS certs for all ingress services automatically using LetsEncrypt .","title":"Networking"},{"location":"home/cluster_overview/#storage","text":"See here for more information about my storage setup. rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage or CephFS storage. Kasten K10 : Data backup and recovery","title":"Storage"},{"location":"home/repo_structure/","text":"Repository structure \u00b6 The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u2514\u2500\u2500 ./crds base directory is the entrypoint to Flux crds directory contains custom resource definitions (CRDs) that need to exist globally in my cluster before anything else exists core directory (depends on crds ) are important infrastructure applications that should never be pruned by Flux apps directory (depends on core ) is where my common applications (grouped by namespace) are placed. Flux will prune resources here if they are not tracked by Git anymore.","title":"Repository structure"},{"location":"home/repo_structure/#repository-structure","text":"The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u2514\u2500\u2500 ./crds base directory is the entrypoint to Flux crds directory contains custom resource definitions (CRDs) that need to exist globally in my cluster before anything else exists core directory (depends on crds ) are important infrastructure applications that should never be pruned by Flux apps directory (depends on core ) is where my common applications (grouped by namespace) are placed. Flux will prune resources here if they are not tracked by Git anymore.","title":"Repository structure"},{"location":"home/tools/","text":"Tools \u00b6 Tool Purpose direnv Sets environment variables and tool environments based on present working directory pre-commit Enforce code consistency and verifies no secrets are pushed stern Tail logs in Kubernetes Repository automation \u00b6 Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date","title":"Tools"},{"location":"home/tools/#tools","text":"Tool Purpose direnv Sets environment variables and tool environments based on present working directory pre-commit Enforce code consistency and verifies no secrets are pushed stern Tail logs in Kubernetes","title":"Tools"},{"location":"home/tools/#repository-automation","text":"Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date","title":"Repository automation"},{"location":"networking/","text":"Networking \u00b6 My current cluster-internal networking is implemented by calico . Running high-available control-plane \u00b6 In order to expose my control-plane on a loadbalanced IP address, I have deployed kube-vip . It is configured to expose a load balanced address to the host IP addresses of my control-plane nodes over BGP. Exposing services on their own IP address \u00b6 Most (http/https) traffic enters my cluster through an Ingress controller. For situations where this is not desirable (e.g. MQTT traffic) or when I need a fixed IP reachable from outside the cluster (e.g. to use in combination with port forwarding) I use metallb in layer2 mode . Using this setup I can define a Service to use a loadBalancerIP , and it will be exposed on my network on that given IP address. Mixed-protocol services \u00b6 I have enabled the MixedProtocolLBService=true feature-gate on my cluster. This means that I can combine UDP and TCP ports on the same Service. BGP \u00b6 Due to the way that BGP works, a node can only set up a single BGP connection to the router. Since I am already running kube-vip in BGP mode and I have a limited number of nodes, I am currently not using BGP mode to expose my services.","title":"Networking"},{"location":"networking/#networking","text":"My current cluster-internal networking is implemented by calico .","title":"Networking"},{"location":"networking/#running-high-available-control-plane","text":"In order to expose my control-plane on a loadbalanced IP address, I have deployed kube-vip . It is configured to expose a load balanced address to the host IP addresses of my control-plane nodes over BGP.","title":"Running high-available control-plane"},{"location":"networking/#exposing-services-on-their-own-ip-address","text":"Most (http/https) traffic enters my cluster through an Ingress controller. For situations where this is not desirable (e.g. MQTT traffic) or when I need a fixed IP reachable from outside the cluster (e.g. to use in combination with port forwarding) I use metallb in layer2 mode . Using this setup I can define a Service to use a loadBalancerIP , and it will be exposed on my network on that given IP address.","title":"Exposing services on their own IP address"},{"location":"networking/#mixed-protocol-services","text":"I have enabled the MixedProtocolLBService=true feature-gate on my cluster. This means that I can combine UDP and TCP ports on the same Service.","title":"Mixed-protocol services"},{"location":"networking/#bgp","text":"Due to the way that BGP works, a node can only set up a single BGP connection to the router. Since I am already running kube-vip in BGP mode and I have a limited number of nodes, I am currently not using BGP mode to expose my services.","title":"BGP"},{"location":"networking/dns/","text":"DNS \u00b6 My DNS setup may seem a bit complicated at first, but it allows for completely automatic management of DNS entries for Services and Ingress objects. Components \u00b6 Traefik \u00b6 Traefik is my cluster Ingress controller. It is set to a LoadbalancerIP so that I can forward a port on my router directly to the Service. CoreDNS with k8s_gateway \u00b6 CoreDNS is running on my VyOS router. I have included the k8s_gateway plugin so that I can connect it directly to my cluster. external-dns \u00b6 external-dns runs in my cluster and is connected to my domains DNS server. It automatically manages records for all my Ingresses that have the external-dns/is-public: \"true\" annotation set. Dynamic DNS \u00b6 In order to keep my WAN IP address up to date on my DNS provider I have deployed a CronJob ( link ) in my cluster that periodically checks and updates those records. How it all works together \u00b6 When I am connected to my home network, my DNS server is set to Adguard Home that is running on my router. I have configured this to forward all requests for my own domain names to the CoreDNS instance that is running on my router. If an Ingress or Service exists for the requested address, k8s_gateway will respond with the IP address that it received from my cluster. If it doesn't exist, it will respond with NXDOMAIN . When I am outside my home network, I will probably use whatever DNS is provided to me. When I request an address for one of my domains, it will query my domains DNS server and will respond with the DNS record that was set by external-dns . Graph \u00b6 This graph shows an example of how I expose one of my services both internally and externally:","title":"DNS"},{"location":"networking/dns/#dns","text":"My DNS setup may seem a bit complicated at first, but it allows for completely automatic management of DNS entries for Services and Ingress objects.","title":"DNS"},{"location":"networking/dns/#components","text":"","title":"Components"},{"location":"networking/dns/#traefik","text":"Traefik is my cluster Ingress controller. It is set to a LoadbalancerIP so that I can forward a port on my router directly to the Service.","title":"Traefik"},{"location":"networking/dns/#coredns-with-k8s_gateway","text":"CoreDNS is running on my VyOS router. I have included the k8s_gateway plugin so that I can connect it directly to my cluster.","title":"CoreDNS with k8s_gateway"},{"location":"networking/dns/#external-dns","text":"external-dns runs in my cluster and is connected to my domains DNS server. It automatically manages records for all my Ingresses that have the external-dns/is-public: \"true\" annotation set.","title":"external-dns"},{"location":"networking/dns/#dynamic-dns","text":"In order to keep my WAN IP address up to date on my DNS provider I have deployed a CronJob ( link ) in my cluster that periodically checks and updates those records.","title":"Dynamic DNS"},{"location":"networking/dns/#how-it-all-works-together","text":"When I am connected to my home network, my DNS server is set to Adguard Home that is running on my router. I have configured this to forward all requests for my own domain names to the CoreDNS instance that is running on my router. If an Ingress or Service exists for the requested address, k8s_gateway will respond with the IP address that it received from my cluster. If it doesn't exist, it will respond with NXDOMAIN . When I am outside my home network, I will probably use whatever DNS is provided to me. When I request an address for one of my domains, it will query my domains DNS server and will respond with the DNS record that was set by external-dns .","title":"How it all works together"},{"location":"networking/dns/#graph","text":"This graph shows an example of how I expose one of my services both internally and externally:","title":"Graph"},{"location":"networking/multus/","text":"Multus \u00b6","title":"Multus"},{"location":"networking/multus/#multus","text":"","title":"Multus"},{"location":"networking/podgateway/","text":"Pod Gateway \u00b6","title":"Pod Gateway"},{"location":"networking/podgateway/#pod-gateway","text":"","title":"Pod Gateway"},{"location":"storage/","text":"Storage \u00b6 Storage in my cluster is handled in a number of ways. The in-cluster storage is provided by a rook-ceph cluster that is running on a number of my nodes. rook-ceph block storage \u00b6 The bulk of my cluster storage relies on my CephBlockPool ( link ). This ensures that my data is replicated across my storage nodes. NFS storage \u00b6 Finally, I have my NAS that exposes several exports over NFS. Given how NFS is a very bad idea for storing application data (see for example this Github issue ) I only use it to store data at rest, such as my personal media files, Linux ISO's, backups, etc.","title":"Storage"},{"location":"storage/#storage","text":"Storage in my cluster is handled in a number of ways. The in-cluster storage is provided by a rook-ceph cluster that is running on a number of my nodes.","title":"Storage"},{"location":"storage/#rook-ceph-block-storage","text":"The bulk of my cluster storage relies on my CephBlockPool ( link ). This ensures that my data is replicated across my storage nodes.","title":"rook-ceph block storage"},{"location":"storage/#nfs-storage","text":"Finally, I have my NAS that exposes several exports over NFS. Given how NFS is a very bad idea for storing application data (see for example this Github issue ) I only use it to store data at rest, such as my personal media files, Linux ISO's, backups, etc.","title":"NFS storage"},{"location":"storage/backups/","text":"Backups \u00b6 Backups of the data that lives inside my cluster are handled by Kasten K10 . This is a commercial backup solution, but is free to use for up to 10 nodes. Please see their website to see if your use case falls under the license agreement. Kasten documentation \u00b6 Others have detailed this tool much better than I can, so I am going to be a bit lazy and just dump a few links here. How to configure Kasten K10 Disaster Recovery Kasten K10 documentation A hands-on lab that goes through the steps of backing up and restoring an application How I back up my data \u00b6 If you have gotten this far, you will now know that K10 introduces the concepts of Profiles and Policies. In summary, a Profile tells K10 where to store the data, and a Policy tells it what you want to back up. My current setup is that I have a single Profile ( link ) pointing to an NFS server. I then have a single Policy ( link ) that schedules snapshots and exports for: a set of namespaces all persistentVolumeClaim resources that have been assigned the label kasten.io/backup-volume: \"true\" Restoring PVCs using Kasten \u00b6 Recovering from a K10 backup involves the following sequence of actions: 1. Create a Kubernetes Secret, k10-dr-secret, using the passphrase provided while enabling DR \u00b6 kubectl create secret generic k10-dr-secret \\ --namespace system-kasten \\ --from-literal key=<passphrase> 2. Install a fresh K10 instance \u00b6 Ensure that Flux has correctly deployed K10 to it's namespace system-kasten 3. Provide NFS information and credentials for the snapshot export location \u00b6 Ensure that Flux has correctly deployed the nfs storage profile and that it's accessible within K10 4. Restoring the K10 backup \u00b6 Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job helm install k10-restore kasten/k10restore --namespace=system-kasten \\ --set sourceClusterID=<source-clusterID> \\ --set profile.name=<location-profile-name> 5. Application recovery \u00b6 Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Backups"},{"location":"storage/backups/#backups","text":"Backups of the data that lives inside my cluster are handled by Kasten K10 . This is a commercial backup solution, but is free to use for up to 10 nodes. Please see their website to see if your use case falls under the license agreement.","title":"Backups"},{"location":"storage/backups/#kasten-documentation","text":"Others have detailed this tool much better than I can, so I am going to be a bit lazy and just dump a few links here. How to configure Kasten K10 Disaster Recovery Kasten K10 documentation A hands-on lab that goes through the steps of backing up and restoring an application","title":"Kasten documentation"},{"location":"storage/backups/#how-i-back-up-my-data","text":"If you have gotten this far, you will now know that K10 introduces the concepts of Profiles and Policies. In summary, a Profile tells K10 where to store the data, and a Policy tells it what you want to back up. My current setup is that I have a single Profile ( link ) pointing to an NFS server. I then have a single Policy ( link ) that schedules snapshots and exports for: a set of namespaces all persistentVolumeClaim resources that have been assigned the label kasten.io/backup-volume: \"true\"","title":"How I back up my data"},{"location":"storage/backups/#restoring-pvcs-using-kasten","text":"Recovering from a K10 backup involves the following sequence of actions:","title":"Restoring PVCs using Kasten"},{"location":"storage/backups/#1-create-a-kubernetes-secret-k10-dr-secret-using-the-passphrase-provided-while-enabling-dr","text":"kubectl create secret generic k10-dr-secret \\ --namespace system-kasten \\ --from-literal key=<passphrase>","title":"1. Create a Kubernetes Secret, k10-dr-secret, using the passphrase provided while enabling DR"},{"location":"storage/backups/#2-install-a-fresh-k10-instance","text":"Ensure that Flux has correctly deployed K10 to it's namespace system-kasten","title":"2. Install a fresh K10 instance"},{"location":"storage/backups/#3-provide-nfs-information-and-credentials-for-the-snapshot-export-location","text":"Ensure that Flux has correctly deployed the nfs storage profile and that it's accessible within K10","title":"3. Provide NFS information and credentials for the snapshot export location"},{"location":"storage/backups/#4-restoring-the-k10-backup","text":"Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job helm install k10-restore kasten/k10restore --namespace=system-kasten \\ --set sourceClusterID=<source-clusterID> \\ --set profile.name=<location-profile-name>","title":"4. Restoring the K10 backup"},{"location":"storage/backups/#5-application-recovery","text":"Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"5. Application recovery"}]}